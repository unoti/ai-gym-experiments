{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison vs State of the Art\n",
    "Let's look at the mechanics of how my solutions compare to the state of the art.\n",
    "[Open Ai Gym Cartpole Leaderboard](https://github.com/openai/gym/wiki/Leaderboard)\n",
    "\n",
    "## Ben Harris\n",
    "[Ben Harris Solution](https://github.com/Ben-C-Harris/Reinforcement-Learning-Pole-Balance/blob/master/kerasPoleBalance.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key differences between [my solution](https://github.com/unoti/ai-gym-experiments/blob/master/cartpole_lab/deeprico.py) [as it stands now](https://github.com/unoti/ai-gym-experiments/commit/ddd7b4e2f9f52ac346534cba3e0e44fb970c4393#diff-658aac99e3937373c82819a6d3ffa7a6) and Ben's:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " | Issue | Ben | Mine |\n",
    " |:-------|-----|------|\n",
    " | Batch size | 10 | 20 |\n",
    " | Model | 24, 24, 2 outputs | 64, 64, 2 outputs |\n",
    " | Q Update | see below | Maybe different? |\n",
    " | Reward at terminal | -1 | 0, and maybe different |\n",
    " | Future reward | amax(..) | max(..) |\n",
    " | Adam parameters | defaults | , beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0 |\n",
    " \n",
    " Is np.amax the same as np.max in this context?\n",
    "\n",
    "Pretty much everything else is the same.  Things that are the same:\n",
    " * **Memory size** We're both using 1,000,000\n",
    " * **Hyperparameters** Learning rate=0.001, gamma=0.95. epsilon_start=1, epsilon_min=0.01, epsilon_decay=0.995 on each step\n",
    " \n",
    "\n",
    "Things to investigate:\n",
    " * **Unreliable random starts.** Is the solution highly vulnerable to getting a bad random seed for the initial network weights?  Do you need to just get lucky?\n",
    "   * I could determine this by downloading their solution and seeing how reliably it converges to \"solved\" and comparing that to mine\n",
    "   * So run theirs a few times and record the results carefully, and repeat the same thing for mine.\n",
    " * **np.amax** vs **np.max**. I've researched this before and determined it's the same thing, but it's worth another look.\n",
    " * **Model**.  My model size is different from Ben's.  The architecture is the same, I think, but the number of nodes per layer is much different.  Could this be the difference?  Unfortunately there's no real way to check how well the model is underfitting/overfitting. Unless I could take a fully-trained champion model, and compare mean squared error on theirs versus mine or something...\n",
    " * **Q Update**.  The Q update functions for Ben's is different from mine.\n",
    " * **Reward at Terminal**.  Ben's function manually tweaks the rewards to -1 somewhere, and mine does not.\n",
    "\n",
    "I'll investigate all of these issues, starting with the Q Update function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q Update Comparison\n",
    "The Q Update step is different in Ben's. This is an issue that's been gnawing on my consciousness for a while now, so I'd like to get to the bottom of it.  Both Ben's update function as well as my own appear to be different from the SARSA update function described in the [Sutton and Barto textbook](http://incompleteideas.net/book/RLbook2018.pdf).  I've tried precisely implementing the one in the book and couldn't make it work exactly as written.\n",
    "\n",
    "Here's the SARSA update formula from *Sutton and Barto* on p. 130:\n",
    "\n",
    "$$ Q(S_t, A_t) = Q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \\right] $$\n",
    "\n",
    " * ùõº: learning rate 0-1\n",
    " * ùõæ: future reward discount rate 0-1\n",
    "\n",
    "\n",
    "\n",
    "Comparing my update function to Ben's, at a glance they look like they are probably (maybe?) algebraically equivalent, but I'm not sure.  Let's work it through to be sure, and see how Ben's compares to mine, and how both of them compare to official SARSA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ben's Q Update Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ben_q_update(self, state, action, reward, next_state, terminal):\n",
    "    q_update = reward # Updated quality score\n",
    "    if not terminal:\n",
    "        q_update = reward + gamma * np.amax(self.model.predict(state_next)[0]) # gamma = discount factor & max predicted quality\n",
    "    q_values = self.model.predict(state)\n",
    "    q_values[0][action] = q_update\n",
    "    self.model.fit(state, q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### My Q Update function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rico_q_update(self, prev_state, prev_action, reward, state, done):\n",
    "    rewards_all = self.model.predict(prev_state)\n",
    "    if done:\n",
    "        future_rewards = 0\n",
    "    else:\n",
    "        future_rewards = np.max(self.model.predict(state))\n",
    "    target_reward = reward + self.gamma * future_rewards\n",
    "    rewards_all[prev_action] = target_reward\n",
    "    self.model.train(prev_state, rewards_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform to make similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ben_q_update(self, state, action, reward, next_state, terminal):\n",
    "    if terminal:\n",
    "        q_update = reward # Updated quality score\n",
    "    else:\n",
    "        q_update = reward + gamma * np.amax(self.model.predict(state_next)[0]) # gamma = discount factor & max predicted quality\n",
    "    q_values = self.model.predict(state)\n",
    "    q_values[0][action] = q_update\n",
    "    self.model.fit(state, q_values)\n",
    "\n",
    "def rico_q_update(self, state, action, reward, next_state, done):\n",
    "    if done:\n",
    "        future_rewards = 0\n",
    "    else:\n",
    "        future_rewards = np.max(self.model.predict(next_state))\n",
    "    q_values = self.model.predict(state)\n",
    "    q_update = reward + self.gamma * future_rewards\n",
    "    q_values[action] = q_update\n",
    "    self.model.train(state, q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They look pretty similar so far, except for what happens in terminal state.  Let's rewrite Rico's to calculate q_update entirely within the if statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rico_q_update(self, state, action, reward, next_state, done):\n",
    "    if done:\n",
    "        q_update = reward\n",
    "    else:\n",
    "        q_update = reward + self.gamma * np.max(self.model.predict(next_state))\n",
    "    q_values = self.model.predict(state)\n",
    "    q_values[action] = q_update\n",
    "    self.model.train(state, q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks exactly the same.  For now I'll conclude that the SARSA update function is the same.  I like the wording of my original function better, because it more intuitively describes the spirit of what we're doing with SARSA update.\n",
    "\n",
    "But note there's another difference in how the updates work which we'll investigate next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward at Terminal\n",
    "\n",
    "Ben has some code that tweaks the reward value to -1 when the episode is over.  Mine doesn't.\n",
    "\n",
    "Ben has this little diddy in the main episode loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_loop():\n",
    "    #...\n",
    "    reward = reward if not terminal else -reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we translate that into code that normal people would rather read, it looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_loop():\n",
    "    if terminal:\n",
    "        reward = -reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This comes before the part where the step is inserted into replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
