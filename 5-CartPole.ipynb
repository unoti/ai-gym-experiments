{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Cart Pole Environment\n",
    "Let's take what we've learned and try it out on the [Cart Pole environment](https://gym.openai.com/envs/CartPole-v1/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "observation = env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea here is there's a cart you can move left or right, and a pole mounted to a joint.  The agent gets rewards each step the pole is above center.  The episode ends if the cart moves too far to the left or right, or if the pole gets more than 15 degrees from vertical.\n",
    "\n",
    "Let's set up some supporting infrastructure to run an agent:\n",
    " * **Episode** will store the history of everything that happened in one episode, which we can use later for *experience replay* training.\n",
    " * **Policy** is a class we'll use that will make all the decisions for an agent. We'll start by implementing a minimal policy that makes random decisions. Later we'll make a better policy class.\n",
    " * **Agent** is everything else that an agent needs to do other than thinking and remembering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Episode:\n",
    "    \"\"\"Tracks the history of what happened in a playthrough for experience replay.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.steps = [] # For each time step, a tuple of (state, action, reward)\n",
    "        self.got_reward = False\n",
    "    \n",
    "    def record_step(self, state, action, reward):\n",
    "        step = (state, action, reward)\n",
    "        self.steps.append(step)\n",
    "        if reward > -1:\n",
    "            self.got_reward = True\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPolicy:            \n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "\n",
    "    def suggest_action(self, state):\n",
    "        return self.env.action_space.sample()\n",
    "    \n",
    "    def update_step(self, prev_state, prev_action, reward, state, action):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    max_steps_per_episode = 1000\n",
    "    \n",
    "    def __init__(self, env, policy):\n",
    "        self.env = env\n",
    "        self.policy = policy\n",
    "    \n",
    "    def run_episode(self, render=False):\n",
    "        episode = Episode()\n",
    "        state = env.reset()\n",
    "        action = None\n",
    "        for i in range(self.max_steps_per_episode):\n",
    "            prev_state = state\n",
    "            prev_action = action\n",
    "            action = self.policy.suggest_action(state)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            episode.record_step(state, action, reward)\n",
    "            if render:\n",
    "                self.env.render()\n",
    "            self.policy.update_step(prev_state, prev_action, reward, state, action)\n",
    "            if done:\n",
    "                break\n",
    "        return episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got a working version of our basic infrastructure that handles everything other than the thinking and learning.  Let's try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(env, RandomPolicy(env))\n",
    "episode = agent.run_episode(render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our basic infrastructure done let's do some actual learning.  The actual learning breaks down into two main components:\n",
    " * Neural net function approximator\n",
    " * Q Learning Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net\n",
    "We're going to need a function approximator. We'll use a neural net for that using the approach desribed in notebook 3, *Neural Net Approximation*.  We will put the function approximator in its own class, because this is a switchable component.  Neural nets are just one way of doing function approximation, and we might want to try some other ways.  It's also nice to keep the function approximation code separate from the Q learning code, to keep the different parts of the code easier to understand.  Plus, keeping them separate makes it easier to test and improve the components  separately from each other.\n",
    "\n",
    "For example, I think I might want to add dropout and L2 regularization to the neural network later.  Having it as a separate component lets me do that and compare the results of the improved approximator versus the vanilla approximator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense # Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "class Approximator:\n",
    "    \"\"\"A function approximator implemented with a deep neural net.\"\"\"\n",
    "    def __init__(self, num_inputs, num_outputs, learning_rate=0.01):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(8, input_shape=(num_inputs,), activation='relu'))\n",
    "        self.model.add(Dense(64, activation='relu'))\n",
    "        self.model.add(Dense(64, activation='relu'))\n",
    "        self.model.add(Dense(num_outputs, activation='linear'))\n",
    "\n",
    "        adam = Adam(lr=0.011, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "        self.model.compile(optimizer=adam, loss='mse', metrics=['mse'])\n",
    "\n",
    "    def predict_multi(self, X_batch):\n",
    "        \"\"\"Predict the outputs for multiple input values at once.\n",
    "        X_batch: an np array of m number of X values to predict, of shape (num_inputs, m)\n",
    "        where m is the number of items you'd like to predict.\n",
    "        \"\"\"\n",
    "        return self.model.predict(X_batch)\n",
    "    \n",
    "    def train_multi(self, X, Y, batch_size=16, epochs=1, verbose=0):\n",
    "        \"\"\"\n",
    "        Train the model with m samples.\n",
    "        X: the input values, of shape (num_inputs, m)\n",
    "        Y: the target values, of shape (num_outputs, m)\n",
    "        \"\"\"\n",
    "        self.model.fit(X, Y, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict a single output given a single input.\n",
    "        X: one set of X values to predict. X shape is (num_inputs,).\n",
    "        \"\"\"\n",
    "        predictions = self.predict_multi(np.array([X])) # An array for m input values.\n",
    "        return predictions[0] # We're just doing one here.\n",
    "    \n",
    "    def train(self, X, Y):\n",
    "        \"\"\"Train a single input/output pair.\n",
    "        X: inputs of shape (num_inputs,)\n",
    "        Y: target outputs of shape (num_inputs,)\"\"\"\n",
    "        batch_X = np.array([X])\n",
    "        batch_Y = np.array([Y])\n",
    "        self.train_multi(batch_X, batch_Y, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03850013, 0.13595746],\n",
       "       [0.00453069, 0.01712409]], dtype=float32)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ap = Approximator(num_inputs=4, num_outputs=2)\n",
    "X = np.array([[1,2,3,4], [1,1,1,1]])\n",
    "ap.predict_multi(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03850013, 0.13595746], dtype=float32)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ap.predict(np.array([1,2,3,4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll train the outputs to be closer to [9,12]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap.train(np.array([1,2,3,4]), np.array([9, 12]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.16851804, 0.45982698], dtype=float32)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ap.predict(np.array([1,2,3,4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do\n",
    " - [ ] Shouldn't the current step number impact the expected sum of rewards from here?\n",
    " - [ ] Shouldn't use epsilon when doing next step prediction in update_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Learning - move this down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQPolicy:            \n",
    "    def __init__(self, env, learning_rate=0.01, epsilon=0.05, gamma=0.9):\n",
    "        \"\"\"epsilon: probability of exploring by selecting a random action\n",
    "        gamma: Discount factor for future rewards. Between 0 - 1.\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma # Discount factor for future rewards.\n",
    "        self.action_list = list(range(self.env.action_space.n)) # All possible actions, like [0,1].\n",
    "        # Function approximator:\n",
    "        # Inputs: state\n",
    "        # Outputs: for each possible action, discounted sum of expected rewards\n",
    "        num_inputs = env.observation_space.shape[0]\n",
    "        num_outputs = len(self.action_list)\n",
    "        self.model = Approximator(num_inputs, num_outputs, learning_rate)\n",
    "\n",
    "    def suggest_action(self, state, epsilon=None):\n",
    "        if epsilon == None:\n",
    "            epsilon = self.epsilon\n",
    "        if np.random.rand() <= epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "\n",
    "        rewards_by_state = self.model.predict(state) # One reward prediction for each possible action.\n",
    "        action = np.argmax(rewards_by_state) # Index of the best action.\n",
    "        return action\n",
    "    \n",
    "    def update_step(self, prev_state, prev_action, reward, state, action):\n",
    "        \"\"\"Update the approximations of the reward function.\n",
    "        See Sutton p. 130 formula 6.7\n",
    "        \"\"\"\n",
    "        q_prev_all = self.model.predict(prev_state) # An array with 1 prediction per possible action.\n",
    "        q_prev = q_prev_all[prev_action] # Predicted value for the action we took.\n",
    "        next_rewards_all = self.model.predict(state) # Rewards for all possible next actions.\n",
    "        next_rewards = next_rewards_all[action] # Rewards for the action we will take.\n",
    "        \n",
    "        q_target = reward + self.gamma * next_rewards - q_prev\n",
    "        print('q_prev=%f next_rewards=%f q_target=%f' % (q_prev, next_rewards, q_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = DeepQPolicy(env)\n",
    "state = env.reset()\n",
    "p.suggest_action(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "state1, reward, done, _ = env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_prev=-0.003622 next_rewards=-0.036567 q_target=0.970711\n"
     ]
    }
   ],
   "source": [
    "prev_state = state\n",
    "prev_action = 1\n",
    "p.update_step(prev_state, prev_action, reward, state1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
